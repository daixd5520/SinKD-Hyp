# MoG-SKD å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸ¯ ä»€ä¹ˆæ˜¯MoG-SKDï¼Ÿ

**MoG-SKD** = **Mixture-of-Geometries Sinkhorn Knowledge Distillation**

ä¸€ä¸ªç”¨äºçŸ¥è¯†è’¸é¦çš„**è‡ªé€‚åº”å¤šå‡ ä½•æ¡†æ¶**ï¼Œæ ¹æ®æ ·æœ¬éš¾åº¦è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„å‡ ä½•ç©ºé—´ã€‚

### æ ¸å¿ƒåˆ›æ–°

```
ç®€å•æ ·æœ¬ â†’ Euclidean / Fisher-Rao (æ ‡å‡†å‡ ä½•)
å›°éš¾æ ·æœ¬ â†’ Hyperbolic (åŒæ›²å‡ ä½•ï¼Œå¤„ç†å±‚æ¬¡ç»“æ„å’Œä¸ç¡®å®šæ€§)
```

---

## ğŸ“¦ å®‰è£…å’Œä½¿ç”¨

### 1. è¿è¡Œæµ‹è¯•éªŒè¯å®‰è£…

```bash
python test_mog_skd.py
```

**é¢„æœŸè¾“å‡º**ï¼š
```
==================================================================
MoG-SKD Test Suite
==================================================================

Testing Fisher-Rao Expert...
  âœ“ Fisher-Rao Expert test passed!

Testing Euclidean Expert...
  âœ“ Euclidean Expert test passed!

Testing Hyperbolic Expert...
  âœ“ Hyperbolic Expert test passed!

Testing Statistical Gating Network...
  âœ“ Statistical Gating test passed!

Testing MoGSKD Unified Framework...
  âœ“ MoGSKD Framework test passed!

...
==================================================================
All tests passed! âœ“
==================================================================

MoG-SKD is ready for KDD submission! ğŸš€
```

---

### 2. å¿«é€Ÿè®­ç»ƒç¤ºä¾‹

```bash
# åŸºç¡€è®­ç»ƒï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
python train_mog_skd.py \
    --dataset_name "copa" \
    --template_name "justify_this" \
    --model_name_or_path "google/t5-small" \
    --teacher_model_path "google/t5-base" \
    --output_dir "./experiments/quick_test" \
    --use_mog_skd \
    --lambda_reg 0.1 \
    --debug \
    --num_train_epochs 1
```

---

### 3. å®Œæ•´è®­ç»ƒï¼ˆç”Ÿäº§çº§ï¼‰

```bash
python train_mog_skd.py \
    --dataset_name "copa" \
    --template_name "justify_this" \
    --model_name_or_path "/path/to/t5-small" \
    --teacher_model_path "/path/to/t5-base-finetuned" \
    --output_dir "./experiments/mog_skd_full" \
    --use_mog_skd \
    --lambda_reg 0.1 \
    --temperature 2.0 \
    --learnable_curvature \
    --hyperbolic_c 1.0 \
    --per_device_train_batch_size 4 \
    --num_train_epochs 10 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 1 \
    --num_warmup_steps 100
```

---

### 4. ç”Ÿæˆè®ºæ–‡å›¾è¡¨

è®­ç»ƒå®Œæˆåï¼Œç”ŸæˆKDDè®ºæ–‡æ‰€éœ€çš„"Money Plot"ï¼š

```bash
python visualize_mog_skd.py \
    --logs_path "./experiments/mog_skd_full/mog_skd_logs.json" \
    --output_dir "./paper_figures"
```

**ç”Ÿæˆçš„å›¾è¡¨**ï¼š
- `money_plot.png` - â­ ä¸“å®¶æƒé‡ vs ä¸ç¡®å®šæ€§ï¼ˆæ ¸å¿ƒå›¾è¡¨ï¼‰
- `expert_losses.png` - å„ä¸“å®¶æŸå¤±æ›²çº¿
- `gating_entropy.png` - é—¨æ§ç‰¹åŒ–è¿‡ç¨‹
- `hyperbolic_curvature.png` - å­¦ä¹ çš„æ›²ç‡è½¨è¿¹

---

## ğŸ”§ ä»£ç ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from mog_skd import MoGSKD

# åˆ›å»ºæ¨¡å‹
mog_skd = MoGSKD(
    T=2.0,              # æ¸©åº¦
    lambda_reg=0.1,     # é—¨æ§ç†µæ­£åˆ™åŒ–ç³»æ•°
    hidden_dim=32,      # é—¨æ§ç½‘ç»œéšè—ç»´åº¦
    learnable_curvature=True  # å¯å­¦ä¹ åŒæ›²æ›²ç‡
)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    student_logits = student_model(batch)
    with torch.no_grad():
        teacher_logits = teacher_model(batch)

    # è®¡ç®—MoG-SKDæŸå¤±
    loss, logs = mog_skd(
        student_logits,
        teacher_logits,
        return_details=True  # è·å–è¯¦ç»†æ—¥å¿—
    )

    # åå‘ä¼ æ’­
    loss.backward()
    optimizer.step()

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    if step % 100 == 0:
        print(f"Loss: {loss.item():.4f}")
        print(f"  Fisher: {logs['weight_fisher']:.2f}")
        print(f"  Euclid: {logs['weight_euclid']:.2f}")
        print(f"  Hyper:  {logs['weight_hyper']:.2f}")
```

---

### é«˜çº§ï¼šä½¿ç”¨é…ç½®ç±»

```python
from mog_skd import MoGSKDConfig

# å®šä¹‰é…ç½®
config = MoGSKDConfig(
    T=2.0,
    lambda_reg=0.1,
    hidden_dim=32,
    use_sinkhorn=False,      # Euclideanä¸“å®¶æ˜¯å¦ç”¨Sinkhorn
    learnable_curvature=True,
    hyperbolic_c=1.0
)

# åˆ›å»ºæ¨¡å‹
mog_skd = config.create_model()

# ä¿å­˜é…ç½®ï¼ˆç”¨äºå®éªŒé‡ç°ï¼‰
import json
with open('config.json', 'w') as f:
    json.dump(config.to_dict(), f, indent=2)
```

---

### åªä½¿ç”¨å•ä¸ªä¸“å®¶ï¼ˆæ¶ˆèå®éªŒï¼‰

```python
from losses.experts import FisherRaoExpert, HyperbolicExpert

# åˆ›å»ºå•ä¸ªä¸“å®¶
expert = HyperbolicExpert(T=2.0, c=1.0)

# è®¡ç®—æŸå¤±
loss = expert(student_logits, teacher_logits)  # [batch_size]

# å¹³å‡æŸå¤±
total_loss = loss.mean()
total_loss.backward()
```

---

## ğŸ“Š ç†è§£è¾“å‡ºæ—¥å¿—

### `logs` å­—å…¸å†…å®¹

```python
{
    # å„ä¸“å®¶æŸå¤±
    'loss_fisher': 0.1234,
    'loss_euclid': 0.2345,
    'loss_hyper': 0.3456,

    # é—¨æ§æƒé‡ï¼ˆæœ€é‡è¦çš„æŒ‡æ ‡ï¼ï¼‰
    'weight_fisher': 0.25,
    'weight_euclid': 0.50,
    'weight_hyper': 0.25,

    # é—¨æ§ç»Ÿè®¡
    'gating_entropy': 0.98,  # è¶Šä½è¶Šç‰¹åŒ–

    # æŸå¤±åˆ†è§£
    'distill_loss': 0.25,
    'reg_loss': 0.098,

    # åŒæ›²æ›²ç‡ï¼ˆå¦‚æœå¯å­¦ä¹ ï¼‰
    'hyperbolic_curvature': 1.05,

    # æ¯æ ·æœ¬æ•°æ®ï¼ˆç”¨äºæ·±åº¦åˆ†æï¼‰
    'per_sample_data': {
        'fisher_loss': tensor([...]),  # [batch_size]
        'euclid_loss': tensor([...]),
        'hyper_loss': tensor([...]),
        'fisher_weight': tensor([...]),
        'euclid_weight': tensor([...]),
        'hyper_weight': tensor([...]),
        'teacher_entropy': tensor([...])  # ä¸ç¡®å®šæ€§
    }
}
```

---

## ğŸ¨ "Money Plot" è§£è¯»

Money Plotï¼ˆexpert_weights_vs_entropy.pngï¼‰æ˜¯KDDè®ºæ–‡çš„æ ¸å¿ƒå›¾è¡¨ã€‚

**Xè½´**ï¼šæ•™å¸ˆé¢„æµ‹ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰
- 0 = å®Œå…¨ç¡®å®šï¼ˆç®€å•æ ·æœ¬ï¼‰
- 1 = å®Œå…¨ä¸ç¡®å®šï¼ˆå›°éš¾æ ·æœ¬ï¼‰

**Yè½´**ï¼šä¸“å®¶æƒé‡ï¼ˆ0-1ï¼‰
- å±•ç¤ºä¸åŒä¸“å®¶åœ¨ä¸åŒéš¾åº¦ä¸‹çš„æƒé‡

**é¢„æœŸè¶‹åŠ¿**ï¼š
```
é«˜æƒé‡ï¼ˆæ¥è¿‘1ï¼‰
    â”‚
    â”‚    Hyperbolic (çº¢)
    â”‚   â•±
    â”‚  â•±  â† å›°éš¾æ ·æœ¬ç”¨åŒæ›²å‡ ä½•
    â”‚ â•±
    â”‚â•±â”€â”€â”€â”€â”€ Euclidean (è“)
    â”‚       â•²
    â”‚        â•² â† ç®€å•æ ·æœ¬ç”¨æ¬§æ°/Fisher-Rao
    â”‚         â•²â”€â”€â”€â”€â”€ Fisher-Rao (ç»¿)
    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0          ç†µ          1
              (ä¸ç¡®å®šæ€§)
```

**è®ºæ–‡å†™æ³•**ï¼š
> "MoG-SKD automatically activates hyperbolic geometry for uncertain samples (high entropy), while simpler geometries handle easy cases. This adaptive selection is achieved through our interpretable statistical gating network."

---

## âš™ï¸ è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

### 1. `lambda_reg` (é—¨æ§ç†µæ­£åˆ™åŒ–)

**èŒƒå›´**: 0.01 - 0.5

- **0.01**: å¼±æ­£åˆ™åŒ– â†’ ä¸“å®¶æƒé‡æ›´å‡åŒ€
- **0.1**: ä¸­ç­‰æ­£åˆ™åŒ–ï¼ˆæ¨èï¼‰â†’ é€‚åº¦ç‰¹åŒ–
- **0.5**: å¼ºæ­£åˆ™åŒ– â†’ å¼ºåˆ¶é€‰æ‹©å•ä¸€ä¸“å®¶

**è°ƒä¼˜ç­–ç•¥**ï¼š
```python
# ä»0.1å¼€å§‹
# å¦‚æœé—¨æ§åå¡Œåˆ°å•ä¸€ä¸“å®¶ â†’ å‡å°lambda_reg
# å¦‚æœé—¨æ§è¿‡äºå‡åŒ€ â†’ å¢å¤§lambda_reg
```

### 2. `temperature` (è’¸é¦æ¸©åº¦)

**èŒƒå›´**: 1.0 - 8.0

- **1.0**: åŸå§‹logitsï¼ˆä¸è½¯åŒ–ï¼‰
- **2.0**: è½»å¾®è½¯åŒ–ï¼ˆæ¨èèµ·ç‚¹ï¼‰
- **4.0+**: å¼ºè½¯åŒ–ï¼Œ smootheræ¢¯åº¦

### 3. `hyperbolic_c` (åˆå§‹æ›²ç‡)

**èŒƒå›´**: 0.5 - 2.0

- **< 1.0**: æ›´å¹³å¦çš„åŒæ›²ç©ºé—´
- **= 1.0**: æ ‡å‡†PoincarÃ©ç›˜ï¼ˆæ¨èï¼‰
- **> 1.0**: æ›´å¼¯æ›²çš„åŒæ›²ç©ºé—´

å¦‚æœ`learnable_curvature=True`ï¼Œè¿™åªæ˜¯ä¸€ä¸ªåˆå§‹å€¼ã€‚

### 4. `hidden_dim` (é—¨æ§ç½‘ç»œå®¹é‡)

**èŒƒå›´**: 16 - 64

- **16**: å°æ¨¡å‹ï¼Œå¿«é€Ÿè®­ç»ƒ
- **32**: æ ‡å‡†ï¼ˆæ¨èï¼‰
- **64**: å¤§æ¨¡å‹ï¼Œæ›´å¼ºè¡¨è¾¾èƒ½åŠ›

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒå¼€å§‹æ—¶losså¾ˆé«˜æˆ–NaNï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. é™ä½å­¦ä¹ ç‡
--learning_rate 5e-5  # ä»1e-4é™ä½

# 2. æ£€æŸ¥logitsèŒƒå›´
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ï¼š
print(f"Student logits range: [{student_logits.min():.2f}, {student_logits.max():.2f}]")

# 3. å‡å°åˆå§‹æ›²ç‡
--hyperbolic_c 0.5  # ä»1.0é™ä½
```

### Q2: é—¨æ§æ€»æ˜¯é€‰æ‹©åŒä¸€ä¸ªä¸“å®¶ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
- `lambda_reg`å¤ªå¤§ â†’ å‡å°åˆ°0.01
- é—¨æ§ç½‘ç»œå®¹é‡ä¸è¶³ â†’ å¢å¤§`hidden_dim`
- æŸä¸ªä¸“å®¶æŸå¤±æ€»æ˜¯æœ€å° â†’ æ£€æŸ¥ä¸“å®¶å®ç°

**è°ƒè¯•ä»£ç **ï¼š
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ‰“å°
print(f"Fisher loss: {logs['loss_fisher']:.4f}")
print(f"Euclid loss: {logs['loss_euclid']:.4f}")
print(f"Hyper loss:  {logs['loss_hyper']:.4f}")
```

### Q3: åŒæ›²ä¸“å®¶æ¢¯åº¦çˆ†ç‚¸ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. æ£€æŸ¥clampè®¾ç½®
# åœ¨ HyperbolicExpert._exp_map() ä¸­ï¼š
v_norm = torch.norm(v, dim=-1, keepdim=True).clamp_min(1e-6)

# 2. é™ä½æ¸©åº¦
--temperature 1.0  # ä»2.0é™ä½

# 3. ä½¿ç”¨å›ºå®šæ›²ç‡
# ä¸ä½¿ç”¨ --learnable_curvature
```

---

## ğŸ“š ä¸‹ä¸€æ­¥

1. **è¿è¡Œæµ‹è¯•**: `python test_mog_skd.py`
2. **å¿«é€Ÿå®éªŒ**: `--debug` æ¨¡å¼å¿«é€ŸéªŒè¯
3. **å®Œæ•´è®­ç»ƒ**: 10+ epochsï¼Œç”Ÿæˆå®Œæ•´æ—¥å¿—
4. **å¯è§†åŒ–**: ç”ŸæˆMoney Plotå’Œå…¶ä»–å›¾è¡¨
5. **æ’°å†™è®ºæ–‡**: å‚è€ƒMOG_SKD_README.mdä¸­çš„KDDæŒ‡å—

---

## ğŸ† KDDæŠ•ç¨¿æ£€æŸ¥æ¸…å•

- [ ] è¿è¡Œå®Œæ•´è®­ç»ƒï¼ˆ10+ epochsï¼‰
- [ ] ç”Ÿæˆæ‰€æœ‰å›¾è¡¨ï¼ˆMoney Plot, è®­ç»ƒåŠ¨æ€, æ¶ˆèï¼‰
- [ ] å¡«å†™æ¶ˆèå®éªŒè¡¨
- [ ] ç¡®è®¤æ•°å­¦ä¸¥è°¨æ€§ï¼ˆå¼•ç”¨æ­£ç¡®ï¼‰
- [ ] éªŒè¯å¯é‡ç°æ€§ï¼ˆéšæœºç§å­ï¼Œé…ç½®æ–‡ä»¶ï¼‰
- [ ] å‡†å¤‡è¡¥å……ææ–™ï¼ˆä»£ç ï¼Œæ•°æ®ï¼‰

**å‡†å¤‡æŠ•ç¨¿ï¼** ğŸš€
